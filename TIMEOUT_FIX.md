# LLM Client Timeout Fix

## Problem

The program was hanging indefinitely when Ollama was not running or when LLM queries took too long. This was caused by:

1. **Blocking synchronous calls** in an async context without proper timeout handling
2. **Connection attempts** that would hang at the TCP level when Ollama was unreachable
3. **No timeout enforcement** at the HTTP client level

## Solution

### 1. Client-Level Timeout

Set a 5-second timeout in the Ollama client initialization:

```python
# src/cognitive_hydraulics/llm/client.py
self._client = ollama.Client(host=self.host, timeout=5.0)
```

This timeout is passed through to the underlying `httpx` client and enforces a 5-second limit on all HTTP operations.

### 2. Retry with Timeout

The `structured_query` method now properly handles timeouts and retries:

```python
try:
    response = client.chat(...)
except (TimeoutError, Exception) as e:
    if "timed out" in str(e).lower():
        if attempt < max_retries:
            if verbose:
                print(f"   âš ï¸  Attempt {attempt + 1} failed: timed out, retrying...")
            continue
        else:
            if verbose:
                print(f"   âœ— LLM query failed after {max_retries + 1} attempts: timed out")
            return None
```

### 3. Better Error Messages

When the LLM is unavailable, the system now provides helpful feedback:

```
âš ï¸  ACT-R failed to generate operators
â„¹ï¸  LLM unavailable. Cannot proceed without rules or LLM.
ðŸ’¡ Tip: Start Ollama with 'ollama serve' for LLM support
```

### 4. Graceful Degradation

The system now operates in "symbolic-only" mode when Ollama is unavailable:
- Rules-based reasoning continues to work
- LLM-based operator generation is skipped
- The program exits cleanly instead of hanging

## Testing

```bash
# Test with Ollama not running
python examples/bug_fix_example.py

# Expected output:
# - Program completes in ~15 seconds (3 retries Ã— 5 second timeout)
# - Helpful error messages about LLM unavailability
# - Clean exit

# Test with Ollama running
ollama serve &
python examples/bug_fix_example.py

# Expected output:
# - Full LLM-powered reasoning
# - Operator generation and evaluation
# - Bug analysis and fix suggestions
```

## Files Modified

1. `src/cognitive_hydraulics/llm/client.py`
   - Added `timeout=5.0` to Client initialization
   - Removed async wrapper (blocking calls with timeout)
   - Improved error handling for timeouts

2. `src/cognitive_hydraulics/engine/cognitive_agent.py`
   - Added helpful error messages when LLM unavailable
   - Clarified symbolic-only mode operation

3. `README.md`
   - Added note about symbolic-only mode
   - Clarified Ollama setup instructions

4. `tests/unit/test_llm_prompts.py`
   - Fixed test assertions for available operators

## Commits

- `008628c` - WIP: Add async timeout handling to LLM client
- `b22146b` - Fix LLM client timeout issues
- `bf2599c` - Add helpful error messages when LLM is unavailable
- `5380cda` - Fix test_llm_prompts test: remove search operator check

## Result

âœ… Program no longer hangs when Ollama is unavailable
âœ… All 211 tests passing
âœ… Clean error messages guide users to start Ollama
âœ… System degrades gracefully to symbolic-only mode

